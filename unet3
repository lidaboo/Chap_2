require 'sys'
require 'torch'
require 'cunn'
require 'nn' 
require 'cudnn'
--matio = require 'matio'
require 'optim'
require 'cutorch'
require 'math'
im = require 'image'
require 'nngraph'
cutorch.setDevice(1)
torch.setdefaulttensortype('torch.FloatTensor')

require 'hdf5'
myFile = hdf5.open('train.hdf5', 'r')
Temp = myFile:read(''):all()
myFile:close()
temp=Temp.data
temp = temp:type('torch.FloatTensor')
inputs = torch.Tensor(temp:size(1),temp:size(2),temp:size(3),temp:size(4)):copy(temp)
temp=Temp.label
temp = temp:type('torch.FloatTensor')
targets = torch.Tensor(temp:size(1),temp:size(2),temp:size(3),temp:size(4)):copy(temp)

myFile = hdf5.open('test.hdf5', 'r')
Temp = myFile:read(''):all()
myFile:close()
temp=Temp.data
temp = temp:type('torch.FloatTensor')
inputs_test = torch.Tensor(temp:size(1),temp:size(2),temp:size(3),temp:size(4)):copy(temp)
temp=Temp.label
temp = temp:type('torch.FloatTensor')
targets_test = torch.Tensor(temp:size(1),temp:size(2),temp:size(3),temp:size(4)):copy(temp):reshape(temp:size(1),temp:size(3),temp:size(4))


n = 64
k = 5
p = 2

input = nn.Identity()()
L1_a = cudnn.ReLU(true)(cudnn.SpatialConvolution(n, n, k,k,1,1,p,p)(cudnn.ReLU(true)(cudnn.SpatialConvolution(1, n, k,k,1,1,p,p)(input))))
L2_a = cudnn.ReLU(true)(nn.SpatialConvolution(2*n, 2*n, k,k,1,1,p,p)(cudnn.ReLU(true)(cudnn.SpatialConvolution(n, 2*n, k,k,1,1,p,p)(cudnn.SpatialMaxPooling(2,2,2,2)(L1_a)))))
L3_a = cudnn.ReLU(true)(cudnn.SpatialConvolution(3*n, 3*n, k,k,1,1,p,p)(cudnn.ReLU(true)(cudnn.SpatialConvolution(2*n, 3*n, k,k,1,1,p,p)(cudnn.SpatialMaxPooling(2,2,2,2)(L2_a)))))
L4_a = cudnn.ReLU(true)(cudnn.SpatialConvolution(4*n, 4*n, k,k,1,1,p,p)(cudnn.ReLU(true)(cudnn.SpatialConvolution(3*n, 4*n, k,k,1,1,p,p)(cudnn.SpatialMaxPooling(2,2,2,2)(L3_a)))))
L5 = cudnn.SpatialFullConvolution(4*n,4*n,k+1,k+1,2,2,p,p)(cudnn.ReLU(true)(cudnn.SpatialConvolution(4*n, 4*n, k,k,1,1,p,p)(cudnn.ReLU(true)(nn.SpatialConvolution(4*n, 4*n, k,k,1,1,p,p)(cudnn.SpatialMaxPooling(2,2,2,2)(L4_a))))))
L4 = cudnn.SpatialFullConvolution(4*n,3*n,k+1,k+1,2,2,p,p)(cudnn.ReLU(true)(cudnn.SpatialConvolution(4*n, 4*n, k,k,1,1,p,p)(cudnn.ReLU(true)(nn.SpatialConvolution(8*n, 4*n, k,k,1,1,p,p)(nn.JoinTable(2)({L5,L4_a}))))))
L3 = cudnn.SpatialFullConvolution(3*n,2*n,k+1,k+1,2,2,p,p)(cudnn.ReLU(true)(cudnn.SpatialConvolution(3*n, 3*n, k,k,1,1,p,p)(cudnn.ReLU(true)(nn.SpatialConvolution(6*n, 3*n, k,k,1,1,p,p)(nn.JoinTable(2)({L4,L3_a}))))))
L2 = cudnn.SpatialFullConvolution(2*n,n,k+1,k+1,2,2,p,p)(cudnn.ReLU(true)(cudnn.SpatialConvolution(2*n, 2*n, k,k,1,1,p,p)(cudnn.ReLU(true)(cudnn.SpatialConvolution(4*n, 2*n, k,k,1,1,p,p)(nn.JoinTable(2)({L3,L2_a}))))))
L1 = cudnn.ReLU(true)(cudnn.SpatialConvolution(n, 9, k,k,1,1,p,p)(cudnn.ReLU(true)(cudnn.SpatialConvolution(2*n, n, k,k,1,1,p,p)(nn.JoinTable(2)({L2,L1_a})))))
model = nn.gModule({input}, {L1})
model:cuda()

 
criterion = cudnn.SpatialCrossEntropyCriterion() 
criterion:cuda()
--tt = model:forward(inputs[1]:cuda())

--kk = criterion:forward(tt,targets[1]:cuda())
cmd = torch.CmdLine()
cmd:text()
cmd:text('Training a simple character-level LSTM language model')
cmd:text()
cmd:text('Options')
cmd:option('-coefL1',0,'L1 norm Coefficient')
cmd:option('-coefL2',0,'L2 norm Coefficient')
cmd:option('-P_L1',0,'L1 penality on activation')
cmd:option('-seed',123,'torch manual random number generator seed')
cmd:option('-nfeat',60,'Number of filters to be considered')
cmd:option('-nfeat2',80,'Number of filters to be considered')
cmd:option('-feat_sz',15,'Each filter size')
cmd:option('-feat_sz2',15,'Each filter size')
cmd:option('-iterations',30,'total no of iterations')
cmd:text()

opt = cmd:parse(arg)

torch.manualSeed(opt.seed)

Cost = 999999
eta = 0
func = function(x)
        if x ~= parameters then
              parameters:copy(x)
        end
        gradParameters:zero()
        f = 0
if neval <54 then
        f_test = 0
end
        neval = neval + 1
if (neval%540) ==0 then
state.learningRate = state.learningRate/10
--state.weightDecay = state.weightDecay/2.5
end
        for i = 1,inputs:size(1) do
            output = model:forward(inputs[{{i},{},{},{}}]:cuda())
            err = criterion:forward(output,targets[i]:cuda())
            f = f + err--torch.Tensor(err):mean()
            df_do = criterion:backward(output,targets[i]:cuda())
            model:backward(inputs[{{i},{},{},{}}]:cuda(), df_do:cuda())
 	    collectgarbage()
        end
        table.insert(train,f/inputs:size(1))
if Cost>f then
Cost = f
eta = neval
end

if (neval%54==0) then     
        model:evaluate()

 out_train = torch.zeros(inputs_test:size(1),inputs_test:size(3),inputs_test:size(4))
        for i = 1,inputs_test:size(1) do
            output = model:forward(inputs_test[{{i},{},{},{}}]:cuda())
            oo,out_train[{{i},{},{}}] = torch.max(output[1]:float(),1)
            collectgarbage()
        end
 
f_test = torch.sum(torch.eq(out_train, targets_test))/(496*512)
        table.insert(test,f_test/inputs_test:size(1))
model:training()
end
--        print(string.format('after %d evaluations J(x) = %f took %f %f', neval, f,  sys:toc(),f_test/inputs_test:size(1)))  
--[[out_train = torch.zeros(inputs_test:size(1),inputs_test:size(3),inputs_test:size(4))
        for i = 1,inputs_test:size(1) do
            output = model:forward(inputs_test[{{i},{},{},{}}]:cuda())
            oo,out_train[{{i},{},{}}] = torch.max(output[1]:float(),1)
            collectgarbage()
        end
f_test = torch.sum(torch.eq(out_train, targets_test))/(496*512)
        table.insert(test,f_test/inputs_test:size(1))
print(f_test/inputs_test:size(1))
end 
]]-- 
        print(string.format('after %d evaluations J(x) = %f took %f %f %f', neval, f,  sys:toc(), Cost,f_test))
      return f/inputs:size(1),gradParameters:div(inputs:size(1))
end

optimState = {maxIter = opt.iterations}
state = {
--maxIter = 100,
learningRate = 1e-4,
   momentum = 0.9,
--dampening = 0,
--weightDecay = 1e-5,
--nesterov = true,
}
optimMethod = optim.adagrad--adadelta--adam--cg--adagrad--adam
sys:tic()
train = {}
test = {}
neval = 0
batch =2
X_train = inputs:clone()
Y_train = targets:clone()
f_test =0
for epoch = 1,300 do
    for temp = 1,X_train:size(1)-batch,batch do
        inputs = X_train[{{temp,temp+batch},{},{},{}}]
        targets = Y_train[{{temp,temp+batch},{},{},{}}]
        parameters,gradParameters = model:getParameters()
        optimMethod(func, parameters,state)
    end
if epoch%50==0 then
state.learningRate = 1e-5
end

--[[ out_train = torch.zeros(inputs_test:size(1),inputs_test:size(3),inputs_test:size(4))
        for i = 1,inputs_test:size(1) do
            output = model:forward(inputs_test[{{i},{},{},{}}]:cuda())
            oo,out_train[{{i},{},{}}] = torch.max(output[1]:float(),1)
            collectgarbage()
        end
f_test = torch.sum(torch.eq(out_train, targets_test))/(496*512)
        table.insert(test,f_test/inputs_test:size(1)) ]]--
print(f_test/inputs_test:size(1))
end
--parameters,gradParameters = model:getParameters()
--optimMethod(func, parameters, optimState)-- <------------------- optimization
--[[for i = 1,10 do
      output = AE:forward(inputs[i]:cuda())
      im.save('Results/Train_' .. i .. '_SRCNN.jpg', output:float():div(255))
      im.save('Results/Train_' .. i .. '_truth.jpg', targets[i]:div(255))
end
for i = 1,17 do
      output = AE:forward(inputs_test[i]:cuda())
      im.save('Results/Test_' .. i .. '_SRCNN.jpg', output:float():div(255))
      im.save('Results/Test_' .. i .. '_truth.jpg', targets_test[i]:div(255))
end]]--
train = torch.Tensor(train)
test = torch.Tensor(test)
torch.save('Model',model)
torch.save('train',train)--torch.save('train.txt',train,'ascii')
torch.save('test',test)

